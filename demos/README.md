
# End-to-End Platform Use-Case Application Demos

**In This Document**

- [Overview](#overview)
- [Stock Trading](#stocks-demo)
- [Predictive Infrastructure Monitoring](#netops-demo)
- [Image Recognition](#image-classification-demo)
- [Natural Language Processing (NLP)](#nlp-demo)
- [Stream Enrichment](#stream-enrich-demo)
- [Network Auto Heal](#network-auto-heal)

<a id="overview"></a>
## Overview

The **demos** tutorials directory contains full end-to-end use-case applications that demonstrate how to use the Iguazio Data Science Platform ("the platform") and related tools to address data science requirements for different industries and implementations.

<a id="stocks-demo"></a>
## Smart Stock Trading

The [**stocks**](stocks/read-stocks.ipynb) demo demonstrates a smart stock-trading application: 
the application reads stock-exchange data from an internet service into a time-series database (TSDB); uses Twitter to analyze the market sentiment on specific stocks, in real time; and saves the data to a platform NoSQL table that is used for generating reports and analyzing and visualizing the data on a Grafana dashboard.

- The stock data is read from Twitter by using the [TwythonStreamer](https://twython.readthedocs.io/en/latest/usage/streaming_api.html) Python wrapper to the Twitter Streaming API, and saved to TSDB and NoSQL tables in the platform.
- Sentiment analysis is done by using the [TextBlob](https://textblob.readthedocs.io/) Python library for natural language processing (NLP).
- The analyzed data is visualized as graphs on a [Grafana](https://grafana.com/grafana) dashboard, which is created from the Jupyter notebook code.
  The data is read from both the TSDB and NoSQL stock tables.

<a id="netops-demo"></a>
## Predictive Infrastructure Monitoring

The [**netops**](netops/01-generator.ipynb) demo demonstrates predictive infrastructure monitoring: the application builds, trains, and deploys a machine-learning model for analyzing and predicting failure in network devices as part of a network operations (NetOps) flow.
The goal is to identify anomalies for device metrics &mdash; such as CPU, memory consumption, or temperature &mdash; which can signify an upcoming issue or failure.

- The model training is sped up by using the [Dask](https://dask.org/) Python library for parallel computing, which extends [pandas](https://pandas.pydata.org/) DataFrames.
- The model prediction is done by using open-source Python libraries &mdash; including [scikit-learn](https://scikit-learn.org) (a.k.a. sklearn), [SciPy](https://www.scipy.org/scipylib/), and [NumPy](http://www.numpy.org/) &mdash; and the gradient boosting ML technique.
- The data is generated by using an open-source generator tool that was written by Iguazio.
  This generator enables users to customize the metrics, data range, and many other parameters, and prepare a data set that's suitable for other similar use cases.

<a id="image-classification-demo"></a>
## Image Recognition

The [**image-classification**](image-classification/keras-cnn-dog-or-cat-classification.ipynb) demo demonstrates image recognition: the application builds and trains an ML model that identifies (recognizes) and classifies images.

- The data is collected by downloading images of dogs and cats from the Iguazio sample data-set AWS bucket.
- The training data for the ML model is prepared by using [pandas](https://pandas.pydata.org/) DataFrames to build a predecition map.
  The data is visualized by using the [Matplotlib](https://matplotlib.org/) Python library.
- An image recognition and classification ML model that identifies the animal type is built and trained by using [Keras](https://keras.io/), [TensorFlow](https://www.tensorflow.org/), and [scikit-learn](https://scikit-learn.org) (a.k.a. sklearn).

<a id="nlp-demo"></a>
## Natural Language Processing (NLP)

The [**nlp**](nlp/nlp-example.ipynb) demo demonstrates natural language processing (NLP): the application processes natural-language textual data &mdash; including spelling correction and sentiment analysis &mdash; and generates a Nuclio serverless function that translates any given text string to another (configurable) language.

- The textual data is collected and processed by using the [TextBlob](https://textblob.readthedocs.io/) Python NLP library. The processing includes spelling correction and sentiment analysis.
- A serverless function that translates text to another language, which is configured in an environment variable, is generated by using the [Nuclio](https://nuclio.io/) framework.

<a id="stream-enrich-demo"></a>
### Stream Enrichment

The [**stream-enrich**](stream-enrich/stream-enrich.ipynb) demo demonstrates a typical stream-based data-engineering pipeline, which is required in many real-world scenarios: data is streamed from an event streaming engine; the data is enriched, in real time, using data from a NoSQL table; the enriched data is saved to an output data stream and then consumed from this stream.

- Car-owner data is streamed into the platform from a simulated streaming engine by using an event-triggered [Nuclio](https://nuclio.io/) serverless function.
- The data is written (ingested) into an input platform stream by using the the platform's [Streaming Web API](https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/streaming-web-api/).
- The input stream data is enriched with additional data, such as the car's color and vendor, and the enriched data is saved to a NoSQL table by using the platform's [NoSQL Web API](https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/nosql-web-api/).
- The Nuclio function writes the enriched data to an output platform data stream by using the platform's Streaming Web API.
- The enriched data is read (consumed) from the output stream by using the platform's Streaming Web API.


<a id="netowrk-auto-heal"></a>
### Netowk Auto Heal

The [**netowrk-auto-heal**](stream-enrich/stream-enrich.ipynb) demo demonstrates a typical telco network data pipeline.Varioud routers and network switches are constantly monitored for metrices latency , utilized bandwidth , packet drops and status changes. A scoring mechanism based on the above metrices marks if a router is healthy or not.
Any unhealthy router is tracked down and the program suggests an alternate path to re-route the traffic to avoid network downtime/congestion because of the faulty router.

- A python script generates network stats in the form of latency , bandwidth utilization , status and packet drops for a set of 100 routers.There are 2 events generated for each router per second for the two interfaces of a router. The data is directly ingested into Iguazio's TSDB data base via REST end points of a [Nuclio](https://nuclio.io/) serverless function.
- The data is also fed to another Nuclio function which assigns a health score to each link in the network.This data is written (ingested) into an Iguazio no-sql table using [NoSQL Web API](https://www.iguazio.com/docs/reference/latest-release/api-reference/web-apis/nosql-web-api/). This table maintains only the latest state of the netowok in real-time.
- A spark batch job is run every 2 minutes on the real-time kv table to generate alternate routes for any unhealthy link detected in the network. It utilizes [Dijkstra's Algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm) to create a digital twin of the network in memory.
- UI <TBD>

